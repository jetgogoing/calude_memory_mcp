#!/usr/bin/env python3
"""
Claude Memory MCP Service - é¡¹ç›®éš”ç¦»æ¼”ç¤º

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨é¡¹ç›®IDå®ç°è®°å¿†éš”ç¦»ã€‚
"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from claude_memory.managers.project_manager import get_project_manager
from claude_memory.models.data_models import (
    ConversationModel,
    MessageModel,
    MessageType,
    SearchQuery,
)
from claude_memory.managers.service_manager import ServiceManager
from claude_memory.processors.semantic_compressor import CompressionRequest, MemoryUnitType


async def demo_project_isolation():
    """æ¼”ç¤ºé¡¹ç›®éš”ç¦»åŠŸèƒ½"""
    print("ğŸš€ Claude Memory - é¡¹ç›®éš”ç¦»æ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    project_manager = get_project_manager()
    service_manager = ServiceManager()
    
    try:
        await service_manager.start_service()
        print("âœ… æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæ¼”ç¤ºé¡¹ç›®
        print("\nğŸ“ åˆ›å»ºæ¼”ç¤ºé¡¹ç›®...")
        
        project_web = project_manager.create_project(
            project_id="web_development",
            name="Webå¼€å‘é¡¹ç›®",
            description="å‰ç«¯å’Œåç«¯å¼€å‘ç›¸å…³çš„è®°å¿†"
        )
        print(f"  âœ“ åˆ›å»ºé¡¹ç›®: {project_web.name} (ID: {project_web.id})")
        
        project_ml = project_manager.create_project(
            project_id="machine_learning",
            name="æœºå™¨å­¦ä¹ é¡¹ç›®",
            description="AIå’Œæœºå™¨å­¦ä¹ ç›¸å…³çš„è®°å¿†"
        )
        print(f"  âœ“ åˆ›å»ºé¡¹ç›®: {project_ml.name} (ID: {project_ml.id})")
        
        # ä¸ºWebé¡¹ç›®æ·»åŠ å¯¹è¯è®°å¿†
        print("\nğŸ’¬ æ·»åŠ é¡¹ç›®ç‰¹å®šçš„å¯¹è¯è®°å¿†...")
        
        web_conversation = ConversationModel(
            project_id="web_development",
            session_id="web_session_1",
            title="Reactç»„ä»¶å¼€å‘è®¨è®º",
            messages=[
                MessageModel(
                    conversation_id="",
                    message_type=MessageType.HUMAN,
                    content="å¦‚ä½•åœ¨Reactä¸­å®ç°è‡ªå®šä¹‰Hookæ¥ç®¡ç†è¡¨å•çŠ¶æ€ï¼Ÿ",
                    timestamp=datetime.utcnow()
                ),
                MessageModel(
                    conversation_id="",
                    message_type=MessageType.ASSISTANT,
                    content="åœ¨Reactä¸­åˆ›å»ºè‡ªå®šä¹‰Hookç®¡ç†è¡¨å•çŠ¶æ€æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å®è·µã€‚å¯ä»¥ä½¿ç”¨useStateå’ŒuseReduceræ¥å®ç°ã€‚",
                    timestamp=datetime.utcnow()
                )
            ],
            started_at=datetime.utcnow()
        )
        
        # å‹ç¼©å¹¶å­˜å‚¨
        if service_manager.semantic_compressor:
            compression_request = CompressionRequest(
                conversation=web_conversation,
                unit_type=MemoryUnitType.QUICK_MU
            )
            result = await service_manager.semantic_compressor.compress_conversation(compression_request)
            print("  âœ“ Webé¡¹ç›®è®°å¿†å·²ä¿å­˜")
        
        # ä¸ºMLé¡¹ç›®æ·»åŠ å¯¹è¯è®°å¿†
        ml_conversation = ConversationModel(
            project_id="machine_learning",
            session_id="ml_session_1",
            title="ç¥ç»ç½‘ç»œæ¶æ„è®¨è®º",
            messages=[
                MessageModel(
                    conversation_id="",
                    message_type=MessageType.HUMAN,
                    content="å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç¥ç»ç½‘ç»œæ¶æ„æ¥å¤„ç†å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Ÿ",
                    timestamp=datetime.utcnow()
                ),
                MessageModel(
                    conversation_id="",
                    message_type=MessageType.ASSISTANT,
                    content="å¯¹äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼ŒCNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰æ˜¯é¦–é€‰ã€‚å¯ä»¥è€ƒè™‘ResNetã€EfficientNetç­‰é¢„è®­ç»ƒæ¨¡å‹ã€‚",
                    timestamp=datetime.utcnow()
                )
            ],
            started_at=datetime.utcnow()
        )
        
        if service_manager.semantic_compressor:
            compression_request = CompressionRequest(
                conversation=ml_conversation,
                unit_type=MemoryUnitType.QUICK_MU
            )
            result = await service_manager.semantic_compressor.compress_conversation(compression_request)
            print("  âœ“ MLé¡¹ç›®è®°å¿†å·²ä¿å­˜")
        
        # ç­‰å¾…å¤„ç†å®Œæˆ
        await asyncio.sleep(2)
        
        # æ¼”ç¤ºæœç´¢éš”ç¦»
        print("\nğŸ” æ¼”ç¤ºé¡¹ç›®è®°å¿†éš”ç¦»...")
        
        # åœ¨Webé¡¹ç›®ä¸­æœç´¢"React"
        print("\nåœ¨Webå¼€å‘é¡¹ç›®ä¸­æœç´¢ 'React':")
        search_query = SearchQuery(query="React Hook", query_type="hybrid", limit=5)
        web_results = await service_manager.search_memories(search_query, project_id="web_development")
        print(f"  æ‰¾åˆ° {web_results.total_count} æ¡ç›¸å…³è®°å¿†")
        for i, result in enumerate(web_results.results[:3], 1):
            print(f"  {i}. {result.memory_unit.title} (ç›¸å…³åº¦: {result.relevance_score:.2f})")
        
        # åœ¨MLé¡¹ç›®ä¸­æœç´¢"React"
        print("\nåœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­æœç´¢ 'React':")
        ml_results = await service_manager.search_memories(search_query, project_id="machine_learning")
        print(f"  æ‰¾åˆ° {ml_results.total_count} æ¡ç›¸å…³è®°å¿†")
        if ml_results.total_count == 0:
            print("  âœ“ æ­£ç¡®ï¼šMLé¡¹ç›®ä¸­æ²¡æœ‰Reactç›¸å…³çš„è®°å¿†")
        
        # åœ¨MLé¡¹ç›®ä¸­æœç´¢"ç¥ç»ç½‘ç»œ"
        print("\nåœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­æœç´¢ 'ç¥ç»ç½‘ç»œ':")
        nn_query = SearchQuery(query="ç¥ç»ç½‘ç»œ CNN", query_type="hybrid", limit=5)
        nn_results = await service_manager.search_memories(nn_query, project_id="machine_learning")
        print(f"  æ‰¾åˆ° {nn_results.total_count} æ¡ç›¸å…³è®°å¿†")
        for i, result in enumerate(nn_results.results[:3], 1):
            print(f"  {i}. {result.memory_unit.title} (ç›¸å…³åº¦: {result.relevance_score:.2f})")
        
        # æ˜¾ç¤ºé¡¹ç›®ç»Ÿè®¡
        print("\nğŸ“Š é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯:")
        projects = project_manager.list_projects()
        
        for project in projects:
            stats = project_manager.get_project_statistics(project.id)
            print(f"\né¡¹ç›®: {project.name} (ID: {project.id})")
            print(f"  - å¯¹è¯æ•°: {stats.get('conversation_count', 0)}")
            print(f"  - è®°å¿†å•å…ƒæ•°: {stats.get('memory_unit_count', 0)}")
            print(f"  - æ€»Tokenæ•°: {stats.get('total_tokens', 0)}")
        
        print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ æ€»ç»“:")
        print("  1. ä¸åŒé¡¹ç›®çš„è®°å¿†æ˜¯å®Œå…¨éš”ç¦»çš„")
        print("  2. æœç´¢åªä¼šè¿”å›æŒ‡å®šé¡¹ç›®å†…çš„è®°å¿†")
        print("  3. æ¯ä¸ªé¡¹ç›®æœ‰ç‹¬ç«‹çš„ç»Ÿè®¡ä¿¡æ¯")
        print("  4. æ”¯æŒè½¯åˆ é™¤å’Œç¡¬åˆ é™¤é¡¹ç›®")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†æ¼”ç¤ºé¡¹ç›®ï¼ˆå¯é€‰ï¼‰
        cleanup = input("\næ˜¯å¦æ¸…ç†æ¼”ç¤ºé¡¹ç›®ï¼Ÿ(y/N): ")
        if cleanup.lower() == 'y':
            print("\nğŸ§¹ æ¸…ç†æ¼”ç¤ºé¡¹ç›®...")
            project_manager.delete_project("web_development", soft_delete=False)
            project_manager.delete_project("machine_learning", soft_delete=False)
            print("  âœ“ æ¼”ç¤ºé¡¹ç›®å·²åˆ é™¤")
        
        await service_manager.stop_service()
        print("\nğŸ‘‹ å†è§ï¼")


if __name__ == "__main__":
    asyncio.run(demo_project_isolation())