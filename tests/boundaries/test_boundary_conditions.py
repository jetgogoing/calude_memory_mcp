"""
Claude Memory MCP æœåŠ¡ - è¾¹ç•Œæ¡ä»¶æµ‹è¯•

æµ‹è¯•å†…å®¹ï¼š
- æå¤§å’Œæå°æ•°æ®é‡å¤„ç†
- ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç å¤„ç†
- ç©ºå€¼å’Œæ— æ•ˆè¾“å…¥å¤„ç†
- ç³»ç»Ÿèµ„æºæé™æµ‹è¯•
- å¹¶å‘æé™æµ‹è¯•
"""

import asyncio
import string
import random
import json
from typing import Dict, Any, List
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
import uuid

import pytest

from claude_memory.managers.service_manager import ServiceManager
from claude_memory.collectors.conversation_collector import ConversationCollector
from claude_memory.processors.semantic_compressor import SemanticCompressor
from claude_memory.retrievers.semantic_retriever import SemanticRetriever
from claude_memory.models.data_models import (
    ConversationModel, 
    MessageModel, 
    MessageType, 
    MemoryUnitModel
)
from claude_memory.processors.semantic_compressor import CompressionRequest
from claude_memory.config.settings import get_settings


class TestDataSizeBoundaries:
    """æ•°æ®å¤§å°è¾¹ç•Œæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_empty_input_handling(self):
        """æµ‹è¯•ç©ºè¾“å…¥å¤„ç†"""
        
        compressor = SemanticCompressor()
        
        # æµ‹è¯•ç©ºå¯¹è¯
        empty_conversation = ConversationModel(
            session_id="empty_test",
            messages=[],
            message_count=0,
            token_count=0,
            title=""
        )
        
        # åº”è¯¥ä¼˜é›…å¤„ç†ç©ºå¯¹è¯
        try:
            request = CompressionRequest(conversation=empty_conversation)
            result = await compressor.compress_conversation(request)
            # å¯èƒ½è¿”å›Noneæˆ–æŠ›å‡ºåˆç†çš„å¼‚å¸¸
            if result is not None:
                assert result.content == "" or result.content is None
        except Exception as e:
            # åº”è¯¥æ˜¯å¯é¢„æœŸçš„å¼‚å¸¸ç±»å‹
            assert "empty" in str(e).lower() or "no content" in str(e).lower()

    @pytest.mark.asyncio 
    async def test_minimal_input_handling(self):
        """æµ‹è¯•æœ€å°è¾“å…¥å¤„ç†"""
        
        compressor = SemanticCompressor()
        
        # æµ‹è¯•å•å­—ç¬¦æ¶ˆæ¯
        minimal_conversation = ConversationModel(
            session_id="minimal_test",
            messages=[
                MessageModel(
                    conversation_id=None,
                    sequence_number=0,
                    message_type=MessageType.HUMAN,
                    content="a",
                    token_count=1
                ),
                MessageModel(
                    conversation_id=None,
                    sequence_number=1,
                    message_type=MessageType.ASSISTANT,
                    content="b",
                    token_count=1
                )
            ],
            message_count=2,
            token_count=2,
            title="min"
        )
        
        # æ¨¡æ‹Ÿå‹ç¼©ç»“æœ
        with patch.object(compressor, '_call_compression_api') as mock_api:
            mock_api.return_value = {
                "summary": "Minimal exchange",
                "key_points": ["single character exchange"],
                "importance_score": 0.1
            }
            
            result = await compressor.compress_conversation(minimal_conversation)
            
            if result:
                assert result.importance_score <= 0.3, "æçŸ­å¯¹è¯é‡è¦æ€§åº”è¯¥å¾ˆä½"
                assert len(result.keywords) >= 1, "å³ä½¿æçŸ­å¯¹è¯ä¹Ÿåº”è¯¥æœ‰å…³é”®è¯"

    @pytest.mark.asyncio
    async def test_maximum_content_handling(self):
        """æµ‹è¯•æœ€å¤§å†…å®¹é‡å¤„ç†"""
        
        compressor = SemanticCompressor()
        
        # åˆ›å»ºè¶…é•¿å†…å®¹ï¼ˆæ¨¡æ‹Ÿè¾¾åˆ°tokené™åˆ¶ï¼‰
        long_content = "This is a very long message. " * 1000  # çº¦30,000å­—ç¬¦
        
        huge_conversation = ConversationModel(
            session_id="huge_test",
            messages=[
                MessageModel(
                    conversation_id=None,
                    sequence_number=0,
                    message_type=MessageType.HUMAN,
                    content="Please explain this topic in detail.",
                    token_count=50
                ),
                MessageModel(
                    conversation_id=None,
                    sequence_number=1,
                    message_type=MessageType.ASSISTANT,
                    content=long_content,
                    token_count=10000  # æ¨¡æ‹Ÿå¤§é‡token
                )
            ],
            message_count=2,
            token_count=10050,
            title="Huge Content Test"
        )
        
        # æµ‹è¯•ç³»ç»Ÿèƒ½å¦å¤„ç†å¤§å†…å®¹
        with patch.object(compressor, '_call_compression_api') as mock_api:
            
            # æ¨¡æ‹Ÿå¯èƒ½çš„å¤„ç†ç­–ç•¥
            if len(long_content) > 20000:  # å†…å®¹è¿‡é•¿
                # ç­–ç•¥1: æˆªæ–­å†…å®¹
                mock_api.return_value = {
                    "summary": "Large content discussion (truncated)",
                    "key_points": ["content was truncated due to size"],
                    "importance_score": 0.7
                }
            else:
                # ç­–ç•¥2: æ­£å¸¸å¤„ç†
                mock_api.return_value = {
                    "summary": "Detailed discussion on requested topic",
                    "key_points": ["comprehensive explanation", "detailed coverage"],
                    "importance_score": 0.8
                }
            
            try:
                result = await compressor.compress_conversation(huge_conversation)
                
                if result:
                    # éªŒè¯å¤§å†…å®¹å¤„ç†
                    assert len(result.content) < len(long_content), "å‹ç¼©åº”è¯¥å‡å°‘å†…å®¹é•¿åº¦"
                    assert result.summary is not None, "åº”è¯¥ç”Ÿæˆæ‘˜è¦"
                    
            except Exception as e:
                # å¯èƒ½å› ä¸ºå†…å®¹è¿‡å¤§è€Œå¤±è´¥ï¼Œè¿™æ˜¯å¯æ¥å—çš„
                error_msg = str(e).lower()
                acceptable_errors = ["too large", "token limit", "content size", "truncated"]
                assert any(err in error_msg for err in acceptable_errors), f"æ„å¤–çš„é”™è¯¯ç±»å‹: {e}"

    @pytest.mark.asyncio
    async def test_unicode_and_special_characters(self):
        """æµ‹è¯•Unicodeå’Œç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        
        compressor = SemanticCompressor()
        
        # åŒ…å«å„ç§ç‰¹æ®Šå­—ç¬¦çš„å¯¹è¯
        special_chars_conversation = ConversationModel(
            session_id="unicode_test",
            messages=[
                MessageModel(
                    conversation_id=None,
                    sequence_number=0,
                    message_type=MessageType.HUMAN,
                    content="Hello! ä½ å¥½ ã“ã‚“ã«ã¡ã¯ ğŸš€ Â¿CÃ³mo estÃ¡s? Ã±Ã¡Ã©Ã­Ã³Ãº â‚¬Â£Â¥ ä¸­æ–‡æµ‹è¯•",
                    token_count=30
                ),
                MessageModel(
                    conversation_id=None,
                    sequence_number=1,
                    message_type=MessageType.ASSISTANT,
                    content="I can help with multiple languages! æˆ‘å¯ä»¥å¸®åŠ©å¤šç§è¯­è¨€ ğŸŒ Special chars: @#$%^&*()_+{}|:<>?[]\\;'\",./ ÄÄ“Ä«ÅÅ«",
                    token_count=40
                )
            ],
            message_count=2,
            token_count=70,
            title="Unicode & Special Characters Test ğŸŒŸ"
        )
        
        with patch.object(compressor, '_call_compression_api') as mock_api:
            mock_api.return_value = {
                "summary": "Multilingual greeting exchange with special characters",
                "key_points": ["multilingual", "unicode support", "special characters"],
                "importance_score": 0.5
            }
            
            try:
                result = await compressor.compress_conversation(special_chars_conversation)
                
                if result:
                    # éªŒè¯Unicodeå­—ç¬¦è¢«æ­£ç¡®å¤„ç†
                    assert result.content is not None, "åº”è¯¥èƒ½å¤„ç†Unicodeå†…å®¹"
                    # æ£€æŸ¥æ˜¯å¦ä¿ç•™äº†ä¸€äº›å¤šè¯­è¨€ç‰¹å¾
                    content_lower = result.content.lower()
                    assert any(word in content_lower for word in ["multilingual", "language", "unicode", "character"])
                    
            except UnicodeError:
                pytest.fail("ä¸åº”è¯¥å‘ç”ŸUnicodeç¼–ç é”™è¯¯")
            except Exception as e:
                # å…¶ä»–å¼‚å¸¸å¯èƒ½æ˜¯APIç›¸å…³çš„
                print(f"Unicodeæµ‹è¯•é‡åˆ°å¼‚å¸¸: {e}")

    @pytest.mark.asyncio
    async def test_malformed_input_handling(self):
        """æµ‹è¯•æ ¼å¼é”™è¯¯è¾“å…¥å¤„ç†"""
        
        collector = ConversationCollector()
        
        # æµ‹è¯•å„ç§æ ¼å¼é”™è¯¯çš„JSON
        malformed_json_logs = [
            '{"incomplete": "json"',  # ä¸å®Œæ•´çš„JSON
            '{"role": "user", "content": }',  # è¯­æ³•é”™è¯¯
            '{"timestamp": "invalid-date", "content": "test"}',  # æ— æ•ˆæ—¥æœŸ
            '',  # ç©ºè¡Œ
            'not json at all',  # éJSONå†…å®¹
            '{"nested": {"very": {"deep": {"structure": {"that": {"goes": {"on": {"forever": "..."}}}}}}}}',  # è¿‡åº¦åµŒå¥—
        ]
        
        # æµ‹è¯•æ¯ç§æ ¼å¼é”™è¯¯
        for malformed_log in malformed_json_logs:
            try:
                # æ¨¡æ‹Ÿè§£ææ—¥å¿—è¡Œ
                with patch.object(collector, '_parse_log_line') as mock_parse:
                    
                    def parse_with_error_handling(line):
                        try:
                            if not line.strip():
                                return None
                            data = json.loads(line)
                            return data
                        except json.JSONDecodeError:
                            return None  # ä¼˜é›…åœ°å¿½ç•¥æ ¼å¼é”™è¯¯
                        except Exception:
                            return None
                    
                    mock_parse.side_effect = lambda x: parse_with_error_handling(malformed_log)
                    
                    result = mock_parse(malformed_log)
                    
                    # åº”è¯¥è¿”å›Noneè€Œä¸æ˜¯å´©æºƒ
                    if malformed_log.strip() and not malformed_log.startswith('not json'):
                        # å¯¹äºçœ‹èµ·æ¥åƒJSONçš„å†…å®¹ï¼Œåº”è¯¥å°è¯•è§£æ
                        pass  # ç»“æœå¯èƒ½æ˜¯Noneï¼Œè¿™æ˜¯OKçš„
            
            except Exception as e:
                # ä¸åº”è¯¥æœ‰æœªæ•è·çš„å¼‚å¸¸
                pytest.fail(f"æ ¼å¼é”™è¯¯è¾“å…¥åº”è¯¥è¢«ä¼˜é›…å¤„ç†ï¼Œä½†å‘ç”Ÿäº†å¼‚å¸¸: {e}")


class TestConcurrencyBoundaries:
    """å¹¶å‘è¾¹ç•Œæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_high_concurrency_stress(self):
        """æµ‹è¯•é«˜å¹¶å‘å‹åŠ›"""
        
        service_manager = ServiceManager()
        
        # åˆ›å»ºå¤§é‡å¹¶å‘ä»»åŠ¡
        concurrent_tasks = 50
        
        async def create_test_task(task_id):
            """åˆ›å»ºæµ‹è¯•ä»»åŠ¡"""
            conversation = ConversationModel(
                session_id=f"stress_test_{task_id}",
                messages=[
                    MessageModel(
                        conversation_id=None,
                        sequence_number=0,
                        message_type=MessageType.HUMAN,
                        content=f"Stress test message {task_id}",
                        token_count=10
                    )
                ],
                message_count=1,
                token_count=10,
                title=f"Stress Test {task_id}"
            )
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            await asyncio.sleep(random.uniform(0.01, 0.1))
            return f"completed_{task_id}"
        
        # æ‰§è¡Œå¹¶å‘å‹åŠ›æµ‹è¯•
        start_time = asyncio.get_event_loop().time()
        
        try:
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = [create_test_task(i) for i in range(concurrent_tasks)]
            
            # è®¾ç½®è¶…æ—¶ä»¥é˜²æ­¢æ— é™ç­‰å¾…
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=30.0
            )
            
            end_time = asyncio.get_event_loop().time()
            execution_time = end_time - start_time
            
            # åˆ†æç»“æœ
            success_count = sum(1 for r in results if isinstance(r, str) and r.startswith("completed_"))
            error_count = sum(1 for r in results if isinstance(r, Exception))
            
            print(f"å¹¶å‘å‹åŠ›æµ‹è¯•ç»“æœ:")
            print(f"  æ€»ä»»åŠ¡: {concurrent_tasks}")
            print(f"  æˆåŠŸ: {success_count}")
            print(f"  é”™è¯¯: {error_count}")
            print(f"  æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
            print(f"  å¹³å‡ååé‡: {concurrent_tasks/execution_time:.2f} ä»»åŠ¡/ç§’")
            
            # éªŒè¯ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸‹çš„è¡¨ç°
            success_rate = success_count / concurrent_tasks
            assert success_rate >= 0.8, f"é«˜å¹¶å‘æˆåŠŸç‡è¿‡ä½: {success_rate:.2%}"
            assert execution_time < 60, "æ‰§è¡Œæ—¶é—´è¿‡é•¿"
            
        except asyncio.TimeoutError:
            pytest.fail("é«˜å¹¶å‘æµ‹è¯•è¶…æ—¶")

    @pytest.mark.asyncio
    async def test_resource_exhaustion_handling(self):
        """æµ‹è¯•èµ„æºè€—å°½å¤„ç†"""
        
        # æ¨¡æ‹Ÿèµ„æºç®¡ç†å™¨
        class ResourceManager:
            def __init__(self, max_resources=10):
                self.max_resources = max_resources
                self.allocated_resources = set()
                self.allocation_count = 0
            
            async def allocate_resource(self):
                if len(self.allocated_resources) >= self.max_resources:
                    raise Exception("Resource pool exhausted")
                
                resource_id = f"resource_{self.allocation_count}"
                self.allocation_count += 1
                self.allocated_resources.add(resource_id)
                return resource_id
            
            async def release_resource(self, resource_id):
                self.allocated_resources.discard(resource_id)
            
            def get_usage_stats(self):
                return {
                    "allocated": len(self.allocated_resources),
                    "max": self.max_resources,
                    "utilization": len(self.allocated_resources) / self.max_resources
                }
        
        resource_manager = ResourceManager(max_resources=5)
        
        # æµ‹è¯•èµ„æºåˆ†é…å’Œé‡Šæ”¾
        allocated_resources = []
        
        # åˆ†é…æ‰€æœ‰å¯ç”¨èµ„æº
        for i in range(5):
            resource = await resource_manager.allocate_resource()
            allocated_resources.append(resource)
        
        # å°è¯•åˆ†é…è¶…å‡ºé™åˆ¶çš„èµ„æº
        with pytest.raises(Exception, match="Resource pool exhausted"):
            await resource_manager.allocate_resource()
        
        # éªŒè¯èµ„æºåˆ©ç”¨ç‡
        stats = resource_manager.get_usage_stats()
        assert stats["utilization"] == 1.0, "èµ„æºåº”è¯¥å®Œå…¨åˆ©ç”¨"
        
        # é‡Šæ”¾èµ„æº
        for resource in allocated_resources[:2]:  # é‡Šæ”¾2ä¸ªèµ„æº
            await resource_manager.release_resource(resource)
        
        # ç°åœ¨åº”è¯¥èƒ½åˆ†é…æ–°èµ„æº
        new_resource = await resource_manager.allocate_resource()
        assert new_resource is not None

    @pytest.mark.asyncio
    async def test_memory_intensive_operations(self):
        """æµ‹è¯•å†…å­˜å¯†é›†æ“ä½œ"""
        
        # æ¨¡æ‹Ÿå†…å­˜å¯†é›†çš„å‘é‡æ“ä½œ
        class MemoryIntensiveProcessor:
            def __init__(self):
                self.memory_usage = []
            
            async def process_large_dataset(self, dataset_size=1000):
                """å¤„ç†å¤§å‹æ•°æ®é›†"""
                
                # æ¨¡æ‹Ÿåˆ›å»ºå¤§å‹å‘é‡
                large_vectors = []
                
                try:
                    for i in range(dataset_size):
                        # åˆ›å»ºé«˜ç»´å‘é‡ï¼ˆæ¨¡æ‹Ÿembeddingï¼‰
                        vector = [random.random() for _ in range(1536)]  # 1536ç»´å‘é‡
                        large_vectors.append(vector)
                        
                        # æ¯100ä¸ªå‘é‡æ£€æŸ¥ä¸€æ¬¡å†…å­˜ä½¿ç”¨
                        if i % 100 == 0:
                            self.memory_usage.append(len(large_vectors))
                            
                            # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
                            await asyncio.sleep(0.01)
                    
                    # æ¨¡æ‹Ÿå‘é‡è®¡ç®—ï¼ˆå†…å­˜å¯†é›†ï¼‰
                    result_count = len(large_vectors)
                    
                    return result_count
                    
                finally:
                    # æ¸…ç†å†…å­˜
                    large_vectors.clear()
            
            def get_peak_memory_usage(self):
                return max(self.memory_usage) if self.memory_usage else 0
        
        processor = MemoryIntensiveProcessor()
        
        # æµ‹è¯•ä¸åŒå¤§å°çš„æ•°æ®é›†
        dataset_sizes = [100, 500, 1000]
        
        for size in dataset_sizes:
            try:
                result = await processor.process_large_dataset(size)
                peak_memory = processor.get_peak_memory_usage()
                
                print(f"æ•°æ®é›†å¤§å° {size}: å¤„ç†å®Œæˆï¼Œå³°å€¼å†…å­˜ä½¿ç”¨: {peak_memory}")
                
                assert result == size, f"åº”è¯¥å¤„ç†æ‰€æœ‰ {size} ä¸ªé¡¹ç›®"
                assert peak_memory > 0, "åº”è¯¥æœ‰å†…å­˜ä½¿ç”¨è®°å½•"
                
            except MemoryError:
                print(f"æ•°æ®é›†å¤§å° {size}: å†…å­˜ä¸è¶³ï¼Œè¿™æ˜¯é¢„æœŸçš„")
                # åœ¨å†…å­˜æœ‰é™çš„ç¯å¢ƒä¸­ï¼Œè¿™æ˜¯å¯æ¥å—çš„
                break
            except Exception as e:
                print(f"æ•°æ®é›†å¤§å° {size}: æ„å¤–é”™è¯¯ {e}")


class TestInputValidationBoundaries:
    """è¾“å…¥éªŒè¯è¾¹ç•Œæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_sql_injection_prevention(self):
        """æµ‹è¯•SQLæ³¨å…¥é˜²æŠ¤"""
        
        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢å‡½æ•°
        class SecureDatabase:
            def __init__(self):
                self.blocked_patterns = [
                    "'; DROP TABLE",
                    "UNION SELECT",
                    "OR 1=1",
                    "'; --",
                    "'; /*"
                ]
            
            async def safe_query(self, user_input):
                """å®‰å…¨çš„æŸ¥è¯¢å‡½æ•°"""
                
                # æ£€æŸ¥å±é™©æ¨¡å¼
                input_upper = user_input.upper()
                for pattern in self.blocked_patterns:
                    if pattern in input_upper:
                        raise ValueError(f"Potentially dangerous input detected: {pattern}")
                
                # æ¨¡æ‹Ÿå‚æ•°åŒ–æŸ¥è¯¢
                sanitized_input = user_input.replace("'", "''")  # ç®€å•çš„è½¬ä¹‰
                return f"SELECT * FROM memories WHERE content LIKE '%{sanitized_input}%'"
        
        secure_db = SecureDatabase()
        
        # æµ‹è¯•æ­£å¸¸è¾“å…¥
        normal_queries = [
            "python programming",
            "machine learning algorithms",
            "web development tips"
        ]
        
        for query in normal_queries:
            result = await secure_db.safe_query(query)
            assert "SELECT" in result, "æ­£å¸¸æŸ¥è¯¢åº”è¯¥æˆåŠŸ"
        
        # æµ‹è¯•æ¶æ„è¾“å…¥
        malicious_queries = [
            "'; DROP TABLE memories; --",
            "test' UNION SELECT * FROM users; --",
            "' OR 1=1; --",
            "test'; DELETE FROM memories; --"
        ]
        
        for malicious_query in malicious_queries:
            with pytest.raises(ValueError, match="Potentially dangerous input"):
                await secure_db.safe_query(malicious_query)

    @pytest.mark.asyncio
    async def test_xss_prevention(self):
        """æµ‹è¯•XSSæ”»å‡»é˜²æŠ¤"""
        
        # æ¨¡æ‹Ÿå†…å®¹æ¸…ç†å‡½æ•°
        class ContentSanitizer:
            def __init__(self):
                self.dangerous_patterns = [
                    "<script>",
                    "javascript:",
                    "onload=",
                    "onerror=",
                    "<iframe",
                    "eval("
                ]
            
            def sanitize_content(self, content):
                """æ¸…ç†æ½œåœ¨çš„XSSå†…å®¹"""
                
                sanitized = content
                
                # ç§»é™¤å±é™©æ ‡ç­¾å’Œå±æ€§
                for pattern in self.dangerous_patterns:
                    if pattern.lower() in sanitized.lower():
                        sanitized = sanitized.replace(pattern, "[REMOVED]")
                        sanitized = sanitized.replace(pattern.upper(), "[REMOVED]")
                        sanitized = sanitized.replace(pattern.capitalize(), "[REMOVED]")
                
                # è½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦
                html_escapes = {
                    '<': '&lt;',
                    '>': '&gt;',
                    '"': '&quot;',
                    "'": '&#x27;',
                    '&': '&amp;'
                }
                
                for char, escape in html_escapes.items():
                    sanitized = sanitized.replace(char, escape)
                
                return sanitized
        
        sanitizer = ContentSanitizer()
        
        # æµ‹è¯•æ¶æ„å†…å®¹
        malicious_contents = [
            "<script>alert('XSS')</script>",
            "<img src=x onerror=alert('XSS')>",
            "javascript:alert('XSS')",
            "<iframe src='javascript:alert(1)'></iframe>",
            "Hello <script>eval('malicious code')</script> World"
        ]
        
        for malicious_content in malicious_contents:
            sanitized = sanitizer.sanitize_content(malicious_content)
            
            # éªŒè¯å±é™©å†…å®¹è¢«ç§»é™¤æˆ–è½¬ä¹‰
            assert "<script>" not in sanitized.lower(), "Scriptæ ‡ç­¾åº”è¯¥è¢«ç§»é™¤"
            assert "javascript:" not in sanitized.lower(), "JavaScriptåè®®åº”è¯¥è¢«ç§»é™¤"
            assert "onerror=" not in sanitized.lower(), "äº‹ä»¶å¤„ç†å™¨åº”è¯¥è¢«ç§»é™¤"
            
            print(f"åŸå§‹: {malicious_content}")
            print(f"æ¸…ç†å: {sanitized}")
            print("---")

    @pytest.mark.asyncio
    async def test_path_traversal_prevention(self):
        """æµ‹è¯•è·¯å¾„éå†æ”»å‡»é˜²æŠ¤"""
        
        # æ¨¡æ‹Ÿå®‰å…¨æ–‡ä»¶è®¿é—®å‡½æ•°
        class SecureFileAccess:
            def __init__(self, base_dir="/safe/directory"):
                self.base_dir = base_dir
                self.dangerous_patterns = [
                    "../",
                    "..\\",
                    "/etc/",
                    "/root/",
                    "C:\\",
                    "~/"
                ]
            
            def validate_file_path(self, file_path):
                """éªŒè¯æ–‡ä»¶è·¯å¾„å®‰å…¨æ€§"""
                
                # æ£€æŸ¥è·¯å¾„éå†æ¨¡å¼
                for pattern in self.dangerous_patterns:
                    if pattern in file_path:
                        raise ValueError(f"Path traversal attempt detected: {pattern}")
                
                # ç¡®ä¿è·¯å¾„åœ¨åŸºç¡€ç›®å½•å†…
                if not file_path.startswith(self.base_dir):
                    safe_path = f"{self.base_dir}/{file_path.lstrip('/')}"
                else:
                    safe_path = file_path
                
                return safe_path
        
        secure_access = SecureFileAccess()
        
        # æµ‹è¯•æ­£å¸¸è·¯å¾„
        safe_paths = [
            "logs/conversation.log",
            "data/memories.json",
            "config/settings.yaml"
        ]
        
        for path in safe_paths:
            result = secure_access.validate_file_path(path)
            assert result.startswith("/safe/directory"), "è·¯å¾„åº”è¯¥åœ¨å®‰å…¨ç›®å½•å†…"
        
        # æµ‹è¯•æ¶æ„è·¯å¾„
        malicious_paths = [
            "../../../etc/passwd",
            "..\\..\\windows\\system32",
            "/etc/shadow",
            "../../root/.ssh/id_rsa",
            "~/../../etc/passwd"
        ]
        
        for malicious_path in malicious_paths:
            with pytest.raises(ValueError, match="Path traversal attempt"):
                secure_access.validate_file_path(malicious_path)


class TestExtremeScenarios:
    """æç«¯åœºæ™¯æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_rapid_fire_requests(self):
        """æµ‹è¯•å¿«é€Ÿè¿ç»­è¯·æ±‚"""
        
        # æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†å™¨
        class RequestProcessor:
            def __init__(self, max_requests_per_second=10):
                self.max_rps = max_requests_per_second
                self.request_times = []
            
            async def process_request(self, request_id):
                current_time = asyncio.get_event_loop().time()
                
                # æ¸…ç†1ç§’å‰çš„è¯·æ±‚è®°å½•
                cutoff_time = current_time - 1.0
                self.request_times = [t for t in self.request_times if t > cutoff_time]
                
                # æ£€æŸ¥è¯·æ±‚é¢‘ç‡
                if len(self.request_times) >= self.max_rps:
                    raise Exception(f"Rate limit exceeded: {len(self.request_times)} requests in last second")
                
                # è®°å½•å½“å‰è¯·æ±‚
                self.request_times.append(current_time)
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                await asyncio.sleep(0.01)
                
                return f"processed_{request_id}"
        
        processor = RequestProcessor(max_requests_per_second=5)
        
        # æµ‹è¯•æ­£å¸¸è¯·æ±‚é¢‘ç‡
        normal_requests = []
        for i in range(3):
            task = processor.process_request(f"normal_{i}")
            normal_requests.append(task)
            await asyncio.sleep(0.3)  # é—´éš”300ms
        
        results = await asyncio.gather(*normal_requests)
        assert all("processed_" in r for r in results), "æ­£å¸¸é¢‘ç‡è¯·æ±‚åº”è¯¥æˆåŠŸ"
        
        # æµ‹è¯•å¿«é€Ÿè¿ç»­è¯·æ±‚ï¼ˆåº”è¯¥è¢«é™åˆ¶ï¼‰
        rapid_requests = []
        for i in range(10):
            task = processor.process_request(f"rapid_{i}")
            rapid_requests.append(task)
            # ä¸ç­‰å¾…ï¼Œç«‹å³å‘é€ä¸‹ä¸€ä¸ªè¯·æ±‚
        
        results = await asyncio.gather(*rapid_requests, return_exceptions=True)
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„è¯·æ±‚
        success_count = sum(1 for r in results if isinstance(r, str))
        error_count = sum(1 for r in results if isinstance(r, Exception))
        
        print(f"å¿«é€Ÿè¯·æ±‚æµ‹è¯•: {success_count} æˆåŠŸ, {error_count} è¢«é™åˆ¶")
        assert error_count > 0, "å¿«é€Ÿè¯·æ±‚åº”è¯¥è¢«é™åˆ¶"

    @pytest.mark.asyncio
    async def test_system_under_extreme_load(self):
        """æµ‹è¯•ç³»ç»Ÿåœ¨æç«¯è´Ÿè½½ä¸‹çš„è¡¨ç°"""
        
        # æ¨¡æ‹Ÿç³»ç»Ÿè´Ÿè½½ç›‘æ§
        class LoadMonitor:
            def __init__(self):
                self.cpu_usage = 0
                self.memory_usage = 0
                self.request_count = 0
                self.error_count = 0
            
            async def simulate_load(self, intensity=1.0):
                """æ¨¡æ‹Ÿç³»ç»Ÿè´Ÿè½½"""
                
                # æ¨¡æ‹ŸCPUå’Œå†…å­˜ä½¿ç”¨éšè´Ÿè½½å¢åŠ 
                self.cpu_usage = min(100, intensity * 30)
                self.memory_usage = min(100, intensity * 25)
                
                # é«˜è´Ÿè½½æ—¶å¢åŠ é”™è¯¯ç‡
                if intensity > 3.0:
                    error_probability = (intensity - 3.0) * 0.2
                    if random.random() < error_probability:
                        self.error_count += 1
                        raise Exception(f"System overload at intensity {intensity}")
                
                self.request_count += 1
                
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´éšè´Ÿè½½å¢åŠ 
                processing_time = 0.01 * intensity
                await asyncio.sleep(processing_time)
                
                return {
                    "cpu": self.cpu_usage,
                    "memory": self.memory_usage,
                    "requests": self.request_count,
                    "errors": self.error_count
                }
        
        monitor = LoadMonitor()
        
        # æµ‹è¯•é€’å¢è´Ÿè½½
        load_levels = [0.5, 1.0, 2.0, 3.0, 4.0, 5.0]
        load_results = []
        
        for load_level in load_levels:
            try:
                # åœ¨æ¯ä¸ªè´Ÿè½½çº§åˆ«ä¸‹è¿è¡Œå¤šä¸ªä»»åŠ¡
                tasks = []
                for i in range(5):
                    task = monitor.simulate_load(load_level)
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # åˆ†æç»“æœ
                success_count = sum(1 for r in results if not isinstance(r, Exception))
                error_count = sum(1 for r in results if isinstance(r, Exception))
                
                load_results.append({
                    "load_level": load_level,
                    "success_count": success_count,
                    "error_count": error_count,
                    "success_rate": success_count / len(results)
                })
                
            except Exception as e:
                print(f"è´Ÿè½½çº§åˆ« {load_level} å¯¼è‡´ç³»ç»Ÿæ•…éšœ: {e}")
        
        # åˆ†æè´Ÿè½½æµ‹è¯•ç»“æœ
        print("æç«¯è´Ÿè½½æµ‹è¯•ç»“æœ:")
        for result in load_results:
            print(f"  è´Ÿè½½ {result['load_level']}: "
                  f"æˆåŠŸç‡ {result['success_rate']:.1%} "
                  f"({result['success_count']}/{result['success_count'] + result['error_count']})")
        
        # éªŒè¯ç³»ç»Ÿåœ¨åˆç†è´Ÿè½½ä¸‹å·¥ä½œ
        low_load_results = [r for r in load_results if r['load_level'] <= 2.0]
        if low_load_results:
            avg_success_rate = sum(r['success_rate'] for r in low_load_results) / len(low_load_results)
            assert avg_success_rate >= 0.9, f"ä½è´Ÿè½½æˆåŠŸç‡åº”è¯¥é«˜äº90%: {avg_success_rate:.1%}"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])