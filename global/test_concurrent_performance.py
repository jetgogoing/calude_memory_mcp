#!/usr/bin/env python3
"""
å¹¶å‘æ€§èƒ½æµ‹è¯•å·¥å…·
æµ‹è¯•Claude Memory MCPæœåŠ¡çš„å¹¶å‘è®¿é—®æ€§èƒ½å’Œç¨³å®šæ€§
"""

import asyncio
import time
import statistics
import sys
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
import yaml

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from global_mcp.concurrent_memory_manager import ConcurrentMemoryManager, create_concurrent_manager


class ConcurrentPerformanceTester:
    """å¹¶å‘æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config = self.load_test_config()
        self.manager = None
        self.test_results = {}
        
    def load_test_config(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        return {
            "database": {
                "url": "sqlite:///test_concurrent_memory.db"
            },
            "vector_store": {
                "url": "http://localhost:6333",
                "collection_name": "test_memories",
                "vector_size": 384
            },
            "concurrency": {
                "max_connections": 20,
                "cache_size": 2000,
                "cache_ttl": 300.0,
                "max_workers": 8
            },
            "memory": {
                "cross_project_sharing": True,
                "project_isolation": False,
                "retention_days": 365
            }
        }
    
    async def setup_test_data(self, num_projects: int = 10, conversations_per_project: int = 5):
        """è®¾ç½®å¤§é‡æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“Š è®¾ç½®æµ‹è¯•æ•°æ®: {num_projects}ä¸ªé¡¹ç›®, æ¯ä¸ªé¡¹ç›®{conversations_per_project}ä¸ªå¯¹è¯...")
        
        conversations_batch = []
        
        for project_idx in range(num_projects):
            project_name = f"test-project-{project_idx:03d}"
            project_path = f"/home/user/projects/{project_name}"
            
            project_context = {
                "project_name": project_name,
                "project_path": project_path,
                "project_type": "test"
            }
            
            for conv_idx in range(conversations_per_project):
                # åˆ›å»ºä¸åŒå¤æ‚åº¦çš„å¯¹è¯
                num_messages = 3 + (conv_idx % 5)  # 3-7æ¡æ¶ˆæ¯
                messages = []
                
                for msg_idx in range(num_messages):
                    if msg_idx % 2 == 0:
                        # ç”¨æˆ·æ¶ˆæ¯
                        content = f"é¡¹ç›®{project_idx}å¯¹è¯{conv_idx}çš„é—®é¢˜{msg_idx}: å…³äºæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å’Œç”¨æˆ·è®¤è¯çš„æŠ€æœ¯æ–¹æ¡ˆ"
                        if project_idx % 3 == 0:
                            content += " Reactå‰ç«¯å¼€å‘"
                        elif project_idx % 3 == 1:
                            content += " Pythonåç«¯å¼€å‘"
                        else:
                            content += " æ•°æ®åˆ†æå¤„ç†"
                        
                        messages.append({
                            "role": "user",
                            "content": content,
                            "type": "user",
                            "metadata": {"message_index": msg_idx}
                        })
                    else:
                        # åŠ©æ‰‹å›å¤
                        content = f"é’ˆå¯¹é—®é¢˜{msg_idx-1}çš„è¯¦ç»†è§£ç­”: æ¨èä½¿ç”¨ç°ä»£åŒ–çš„æŠ€æœ¯æ ˆï¼ŒåŒ…æ‹¬æ•°æ®åº“è¿æ¥æ± ã€ç¼“å­˜ç­–ç•¥ã€ç”¨æˆ·è®¤è¯æœ€ä½³å®è·µã€‚"
                        content += f" å…·ä½“å®ç°å¯ä»¥å‚è€ƒé¡¹ç›®{project_idx}ä¸­çš„ç»éªŒã€‚"
                        
                        messages.append({
                            "role": "assistant", 
                            "content": content,
                            "type": "assistant",
                            "metadata": {"message_index": msg_idx}
                        })
                
                conversation_data = {
                    "title": f"é¡¹ç›®{project_idx} - æŠ€æœ¯è®¨è®º{conv_idx}",
                    "messages": messages,
                    "metadata": {
                        "test_data": True,
                        "project_index": project_idx,
                        "conversation_index": conv_idx
                    }
                }
                
                conversations_batch.append((conversation_data, project_context))
        
        # æ‰¹é‡å­˜å‚¨æ‰€æœ‰å¯¹è¯
        start_time = time.time()
        conversation_ids = await self.manager.store_conversation_batch(conversations_batch)
        storage_time = time.time() - start_time
        
        print(f"âœ“ æ•°æ®è®¾ç½®å®Œæˆ: {len(conversation_ids)}ä¸ªå¯¹è¯, è€—æ—¶: {storage_time:.2f}ç§’")
        return len(conversation_ids)
    
    async def test_concurrent_search(self, num_concurrent: int = 50, num_queries: int = 100) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘æœç´¢æ€§èƒ½"""
        print(f"\nğŸ” æµ‹è¯•å¹¶å‘æœç´¢: {num_concurrent}ä¸ªå¹¶å‘è¯·æ±‚, æ€»å…±{num_queries}æ¬¡æŸ¥è¯¢")
        
        search_queries = [
            "æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–",
            "ç”¨æˆ·è®¤è¯",
            "Reactå‰ç«¯",
            "Pythonåç«¯", 
            "æ•°æ®åˆ†æ",
            "æŠ€æœ¯æ–¹æ¡ˆ",
            "ç¼“å­˜ç­–ç•¥",
            "è¿æ¥æ± ",
            "æœ€ä½³å®è·µ",
            "é¡¹ç›®ç»éªŒ"
        ]
        
        async def single_search(query_id: int):
            """å•ä¸ªæœç´¢ä»»åŠ¡"""
            query = search_queries[query_id % len(search_queries)]
            start_time = time.time()
            
            try:
                results = await self.manager.search_memories_concurrent(
                    query=query,
                    limit=10,
                    use_cache=True
                )
                duration = time.time() - start_time
                return {
                    "query_id": query_id,
                    "query": query,
                    "duration": duration,
                    "result_count": len(results),
                    "success": True,
                    "error": None
                }
            except Exception as e:
                duration = time.time() - start_time
                return {
                    "query_id": query_id,
                    "query": query,
                    "duration": duration,
                    "result_count": 0,
                    "success": False,
                    "error": str(e)
                }
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for i in range(num_queries):
            task = single_search(i)
            tasks.append(task)
            
            # æ§åˆ¶å¹¶å‘æ•°é‡
            if len(tasks) >= num_concurrent:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                tasks = []
                
                # å¤„ç†ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        print(f"æœç´¢ä»»åŠ¡å¼‚å¸¸: {result}")
        
        # å¤„ç†å‰©ä½™ä»»åŠ¡
        if tasks:
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        start_time = time.time()
        all_tasks = [single_search(i) for i in range(num_queries)]
        results = await asyncio.gather(*all_tasks)
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        successful_results = [r for r in results if r["success"]]
        failed_results = [r for r in results if not r["success"]]
        
        durations = [r["duration"] for r in successful_results]
        result_counts = [r["result_count"] for r in successful_results]
        
        analysis = {
            "total_queries": num_queries,
            "successful_queries": len(successful_results),
            "failed_queries": len(failed_results),
            "success_rate": len(successful_results) / num_queries,
            "total_time": total_time,
            "queries_per_second": num_queries / total_time,
            "avg_duration": statistics.mean(durations) if durations else 0,
            "median_duration": statistics.median(durations) if durations else 0,
            "min_duration": min(durations) if durations else 0,
            "max_duration": max(durations) if durations else 0,
            "avg_result_count": statistics.mean(result_counts) if result_counts else 0,
            "errors": [r["error"] for r in failed_results]
        }
        
        print(f"æœç´¢æµ‹è¯•ç»“æœ:")
        print(f"  æˆåŠŸç‡: {analysis['success_rate']:.1%}")
        print(f"  æ€»è€—æ—¶: {analysis['total_time']:.2f}ç§’")
        print(f"  æŸ¥è¯¢é€Ÿç‡: {analysis['queries_per_second']:.1f} QPS")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {analysis['avg_duration']*1000:.1f}ms")
        print(f"  å“åº”æ—¶é—´ä¸­ä½æ•°: {analysis['median_duration']*1000:.1f}ms")
        print(f"  å¹³å‡ç»“æœæ•°: {analysis['avg_result_count']:.1f}")
        
        return analysis
    
    async def test_mixed_workload(self, duration_seconds: int = 60) -> Dict[str, Any]:
        """æµ‹è¯•æ··åˆå·¥ä½œè´Ÿè½½"""
        print(f"\nğŸ”„ æµ‹è¯•æ··åˆå·¥ä½œè´Ÿè½½: {duration_seconds}ç§’æŒç»­å‹åŠ›æµ‹è¯•")
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        stats = {
            "search_requests": 0,
            "recent_requests": 0,
            "total_requests": 0,
            "errors": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        async def search_worker():
            """æœç´¢å·¥ä½œè´Ÿè½½"""
            search_queries = ["æ•°æ®åº“", "è®¤è¯", "React", "Python", "ä¼˜åŒ–"]
            while time.time() < end_time:
                try:
                    query = search_queries[stats["search_requests"] % len(search_queries)]
                    await self.manager.search_memories_concurrent(query, limit=5)
                    stats["search_requests"] += 1
                    stats["total_requests"] += 1
                except Exception:
                    stats["errors"] += 1
                await asyncio.sleep(0.1)  # å°å»¶è¿Ÿ
        
        async def recent_worker():
            """æœ€è¿‘å¯¹è¯å·¥ä½œè´Ÿè½½"""
            while time.time() < end_time:
                try:
                    await self.manager.get_recent_conversations_concurrent(limit=10)
                    stats["recent_requests"] += 1
                    stats["total_requests"] += 1
                except Exception:
                    stats["errors"] += 1
                await asyncio.sleep(0.2)  # ä¸åŒçš„å»¶è¿Ÿæ¨¡å¼
        
        # å¯åŠ¨å¤šä¸ªå·¥ä½œè´Ÿè½½
        workers = []
        for _ in range(10):  # 10ä¸ªæœç´¢å·¥ä½œè€…
            workers.append(asyncio.create_task(search_worker()))
        for _ in range(5):   # 5ä¸ªæœ€è¿‘å¯¹è¯å·¥ä½œè€…
            workers.append(asyncio.create_task(recent_worker()))
        
        # ç­‰å¾…æµ‹è¯•å®Œæˆ
        await asyncio.sleep(duration_seconds + 1)
        
        # å–æ¶ˆæ‰€æœ‰å·¥ä½œè€…
        for worker in workers:
            worker.cancel()
        
        # ç­‰å¾…å·¥ä½œè€…æ¸…ç†
        await asyncio.gather(*workers, return_exceptions=True)
        
        # è®¡ç®—ç»“æœ
        actual_duration = time.time() - start_time
        total_qps = stats["total_requests"] / actual_duration
        error_rate = stats["errors"] / max(stats["total_requests"], 1)
        
        print(f"æ··åˆè´Ÿè½½æµ‹è¯•ç»“æœ:")
        print(f"  æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"  æœç´¢è¯·æ±‚: {stats['search_requests']}")
        print(f"  æœ€è¿‘å¯¹è¯è¯·æ±‚: {stats['recent_requests']}")
        print(f"  é”™è¯¯æ•°: {stats['errors']}")
        print(f"  é”™è¯¯ç‡: {error_rate:.1%}")
        print(f"  æ€»QPS: {total_qps:.1f}")
        
        return {
            **stats,
            "duration": actual_duration,
            "total_qps": total_qps,
            "error_rate": error_rate
        }
    
    async def test_cache_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        print(f"\nğŸ’¾ æµ‹è¯•ç¼“å­˜æ€§èƒ½")
        
        # æ‰§è¡Œä¸€äº›æœç´¢æ¥å¡«å……ç¼“å­˜
        queries = ["æ•°æ®åº“", "è®¤è¯", "React", "Python", "ä¼˜åŒ–"]
        
        # ç¬¬ä¸€è½®ï¼šç¼“å­˜æœªå‘½ä¸­
        print("ç¬¬ä¸€è½®æœç´¢ (ç¼“å­˜æœªå‘½ä¸­)...")
        first_round_times = []
        for query in queries:
            start_time = time.time()
            await self.manager.search_memories_concurrent(query, use_cache=False)
            duration = time.time() - start_time
            first_round_times.append(duration)
        
        # ç¬¬äºŒè½®ï¼šç¼“å­˜å‘½ä¸­
        print("ç¬¬äºŒè½®æœç´¢ (ç¼“å­˜å‘½ä¸­)...")
        second_round_times = []
        for query in queries:
            start_time = time.time()
            await self.manager.search_memories_concurrent(query, use_cache=True)
            duration = time.time() - start_time
            second_round_times.append(duration)
        
        avg_uncached = statistics.mean(first_round_times)
        avg_cached = statistics.mean(second_round_times)
        speedup = avg_uncached / avg_cached if avg_cached > 0 else 0
        
        print(f"ç¼“å­˜æ€§èƒ½ç»“æœ:")
        print(f"  æœªç¼“å­˜å¹³å‡æ—¶é—´: {avg_uncached*1000:.1f}ms")
        print(f"  ç¼“å­˜å‘½ä¸­å¹³å‡æ—¶é—´: {avg_cached*1000:.1f}ms")
        print(f"  ç¼“å­˜åŠ é€Ÿæ¯”: {speedup:.1f}x")
        
        return {
            "avg_uncached_time": avg_uncached,
            "avg_cached_time": avg_cached,
            "cache_speedup": speedup,
            "first_round_times": first_round_times,
            "second_round_times": second_round_times
        }
    
    async def test_connection_pool_efficiency(self) -> Dict[str, Any]:
        """æµ‹è¯•è¿æ¥æ± æ•ˆç‡"""
        print(f"\nğŸ”Œ æµ‹è¯•è¿æ¥æ± æ•ˆç‡")
        
        # åˆ›å»ºå¤§é‡å¹¶å‘è¯·æ±‚æ¥æµ‹è¯•è¿æ¥æ± 
        async def db_task():
            try:
                await self.manager.search_memories_concurrent("test", limit=1)
                return True
            except Exception:
                return False
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [5, 10, 20, 50]
        results = {}
        
        for level in concurrency_levels:
            print(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            start_time = time.time()
            
            tasks = [db_task() for _ in range(level)]
            task_results = await asyncio.gather(*tasks)
            
            duration = time.time() - start_time
            success_count = sum(task_results)
            success_rate = success_count / level
            
            results[level] = {
                "duration": duration,
                "success_count": success_count,
                "success_rate": success_rate,
                "requests_per_second": level / duration
            }
            
            print(f"  è€—æ—¶: {duration:.2f}s, æˆåŠŸç‡: {success_rate:.1%}")
        
        return results
    
    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹Claude Memory MCPå¹¶å‘æ€§èƒ½ç»¼åˆæµ‹è¯•")
        print("=" * 80)
        
        try:
            # åˆå§‹åŒ–ç®¡ç†å™¨
            print("âš™ï¸  åˆå§‹åŒ–å¹¶å‘è®°å¿†ç®¡ç†å™¨...")
            self.manager = create_concurrent_manager(self.config)
            await self.manager.initialize()
            print("âœ“ ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # è®¾ç½®æµ‹è¯•æ•°æ®
            num_conversations = await self.setup_test_data(
                num_projects=20, 
                conversations_per_project=10
            )
            
            # æ‰§è¡Œå„é¡¹æµ‹è¯•
            print(f"\nå¼€å§‹æ€§èƒ½æµ‹è¯• (æ•°æ®é›†: {num_conversations}ä¸ªå¯¹è¯)...")
            
            # 1. å¹¶å‘æœç´¢æµ‹è¯•
            search_results = await self.test_concurrent_search(
                num_concurrent=30, 
                num_queries=200
            )
            
            # 2. æ··åˆå·¥ä½œè´Ÿè½½æµ‹è¯•
            mixed_results = await self.test_mixed_workload(duration_seconds=30)
            
            # 3. ç¼“å­˜æ€§èƒ½æµ‹è¯•
            cache_results = await self.test_cache_performance()
            
            # 4. è¿æ¥æ± æ•ˆç‡æµ‹è¯•
            pool_results = await self.test_connection_pool_efficiency()
            
            # 5. è·å–ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡
            perf_stats = await self.manager.get_performance_stats()
            
            # 6. å¥åº·æ£€æŸ¥
            health_check = await self.manager.health_check_concurrent()
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            await self.generate_performance_report({
                "test_config": self.config,
                "data_setup": {"num_conversations": num_conversations},
                "concurrent_search": search_results,
                "mixed_workload": mixed_results,
                "cache_performance": cache_results,
                "connection_pool": pool_results,
                "system_stats": perf_stats,
                "health_check": health_check
            })
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        finally:
            if self.manager:
                await self.manager.close()
    
    async def generate_performance_report(self, test_data: Dict[str, Any]):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š...")
        
        report = {
            "test_timestamp": datetime.now().isoformat(),
            "test_summary": {
                "concurrent_search_qps": test_data["concurrent_search"]["queries_per_second"],
                "search_success_rate": test_data["concurrent_search"]["success_rate"],
                "avg_response_time_ms": test_data["concurrent_search"]["avg_duration"] * 1000,
                "cache_speedup": test_data["cache_performance"]["cache_speedup"],
                "mixed_workload_qps": test_data["mixed_workload"]["total_qps"],
                "system_health": test_data["health_check"]["status"]
            },
            "detailed_results": test_data
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path(__file__).parent / "test_results" / f"concurrent_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # è¾“å‡ºå…³é”®æŒ‡æ ‡æ€»ç»“
        print(f"\nğŸ¯ å¹¶å‘æ€§èƒ½æµ‹è¯•æ€»ç»“:")
        print(f"  âœ… å¹¶å‘æœç´¢QPS: {report['test_summary']['concurrent_search_qps']:.1f}")
        print(f"  âœ… æœç´¢æˆåŠŸç‡: {report['test_summary']['search_success_rate']:.1%}")
        print(f"  âœ… å¹³å‡å“åº”æ—¶é—´: {report['test_summary']['avg_response_time_ms']:.1f}ms")
        print(f"  âœ… ç¼“å­˜åŠ é€Ÿæ¯”: {report['test_summary']['cache_speedup']:.1f}x")
        print(f"  âœ… æ··åˆè´Ÿè½½QPS: {report['test_summary']['mixed_workload_qps']:.1f}")
        print(f"  âœ… ç³»ç»Ÿå¥åº·çŠ¶æ€: {report['test_summary']['system_health']}")
        
        # æ€§èƒ½åŸºå‡†è¯„ä¼°
        print(f"\nğŸ“ˆ æ€§èƒ½åŸºå‡†è¯„ä¼°:")
        if report['test_summary']['concurrent_search_qps'] >= 50:
            print("  ğŸŸ¢ å¹¶å‘æœç´¢æ€§èƒ½: ä¼˜ç§€ (â‰¥50 QPS)")
        elif report['test_summary']['concurrent_search_qps'] >= 20:
            print("  ğŸŸ¡ å¹¶å‘æœç´¢æ€§èƒ½: è‰¯å¥½ (â‰¥20 QPS)")
        else:
            print("  ğŸŸ  å¹¶å‘æœç´¢æ€§èƒ½: éœ€è¦ä¼˜åŒ– (<20 QPS)")
        
        if report['test_summary']['avg_response_time_ms'] <= 100:
            print("  ğŸŸ¢ å“åº”æ—¶é—´: ä¼˜ç§€ (â‰¤100ms)")
        elif report['test_summary']['avg_response_time_ms'] <= 500:
            print("  ğŸŸ¡ å“åº”æ—¶é—´: è‰¯å¥½ (â‰¤500ms)")
        else:
            print("  ğŸŸ  å“åº”æ—¶é—´: éœ€è¦ä¼˜åŒ– (>500ms)")
        
        if report['test_summary']['cache_speedup'] >= 5:
            print("  ğŸŸ¢ ç¼“å­˜æ•ˆæœ: ä¼˜ç§€ (â‰¥5xåŠ é€Ÿ)")
        elif report['test_summary']['cache_speedup'] >= 2:
            print("  ğŸŸ¡ ç¼“å­˜æ•ˆæœ: è‰¯å¥½ (â‰¥2xåŠ é€Ÿ)")
        else:
            print("  ğŸŸ  ç¼“å­˜æ•ˆæœ: éœ€è¦ä¼˜åŒ– (<2xåŠ é€Ÿ)")


async def main():
    """ä¸»å‡½æ•°"""
    tester = ConcurrentPerformanceTester()
    
    try:
        await tester.run_comprehensive_test()
        print(f"\nğŸ‰ Claude Memory MCPå¹¶å‘æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())