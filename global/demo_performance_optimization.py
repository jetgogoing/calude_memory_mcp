#!/usr/bin/env python3
"""
æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½æ¼”ç¤º
å±•ç¤ºClaude Memory MCPæœåŠ¡çš„å¹¶å‘ä¼˜åŒ–ã€ç¼“å­˜ç­–ç•¥å’Œè‡ªåŠ¨æ‰©å±•èƒ½åŠ›
"""

import asyncio
import time
import statistics
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from global_mcp.concurrent_memory_manager import ConcurrentMemoryManager, create_concurrent_manager


class PerformanceOptimizationDemo:
    """æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º"""
    
    def __init__(self):
        self.config = self.load_demo_config()
        self.manager = None
        
    def load_demo_config(self) -> Dict[str, Any]:
        """åŠ è½½æ¼”ç¤ºé…ç½®"""
        return {
            "database": {
                "url": "sqlite:///demo_performance.db"
            },
            "vector_store": {
                "url": "http://localhost:6333",
                "collection_name": "demo_memories"
            },
            "concurrency": {
                "max_connections": 15,
                "cache_size": 1000,
                "cache_ttl": 120.0,
                "max_workers": 6
            },
            "memory": {
                "cross_project_sharing": True,
                "project_isolation": False,
                "retention_days": 365
            }
        }
    
    async def setup_demo_data(self):
        """è®¾ç½®æ¼”ç¤ºæ•°æ®"""
        print("ğŸ“Š è®¾ç½®æ€§èƒ½ä¼˜åŒ–æ¼”ç¤ºæ•°æ®...")
        
        # åˆ›å»ºæ¼”ç¤ºå¯¹è¯æ•°æ®
        conversations_data = []
        
        for i in range(50):  # 50ä¸ªå¯¹è¯
            project_context = {
                "project_name": f"project-{i % 5}",  # 5ä¸ªé¡¹ç›®
                "project_path": f"/demo/project-{i % 5}",
                "project_type": "demo"
            }
            
            conversation_data = {
                "title": f"æ¼”ç¤ºå¯¹è¯ {i}",
                "messages": [
                    {
                        "role": "user",
                        "content": f"è¿™æ˜¯æ¼”ç¤ºé—®é¢˜ {i}ï¼Œå…³äºæ€§èƒ½ä¼˜åŒ–ã€ç¼“å­˜ç­–ç•¥å’Œå¹¶å‘å¤„ç†çš„è®¨è®ºã€‚"
                    },
                    {
                        "role": "assistant", 
                        "content": f"å…³äºé—®é¢˜ {i} çš„å›ç­”ï¼šæ€§èƒ½ä¼˜åŒ–éœ€è¦è€ƒè™‘å¤šä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬æ•°æ®åº“ç´¢å¼•ã€ç¼“å­˜æœºåˆ¶ã€è¿æ¥æ± ç®¡ç†ç­‰ã€‚"
                    }
                ],
                "metadata": {"demo": True, "batch": i // 10}
            }
            
            conversations_data.append((conversation_data, project_context))
        
        # æ‰¹é‡å­˜å‚¨
        start_time = time.time()
        conversation_ids = await self.manager.store_conversation_batch(conversations_data)
        storage_time = time.time() - start_time
        
        print(f"âœ“ æ¼”ç¤ºæ•°æ®åˆ›å»ºå®Œæˆ: {len(conversation_ids)}ä¸ªå¯¹è¯, è€—æ—¶: {storage_time:.2f}ç§’")
        return len(conversation_ids)
    
    async def demo_cache_optimization(self):
        """æ¼”ç¤ºç¼“å­˜ä¼˜åŒ–"""
        print("\nğŸ’¾ ç¼“å­˜ä¼˜åŒ–æ¼”ç¤º")
        print("-" * 50)
        
        test_queries = ["æ€§èƒ½ä¼˜åŒ–", "ç¼“å­˜ç­–ç•¥", "å¹¶å‘å¤„ç†", "æ•°æ®åº“", "è¿æ¥æ± "]
        
        # ç¬¬ä¸€è½®ï¼šæ— ç¼“å­˜æŸ¥è¯¢
        print("ğŸ” ç¬¬ä¸€è½®æŸ¥è¯¢ (ç¼“å­˜æœªå‘½ä¸­):")
        uncached_times = []
        for query in test_queries:
            start_time = time.time()
            results = await self.manager.search_memories_concurrent(query, use_cache=False)
            duration = time.time() - start_time
            uncached_times.append(duration)
            print(f"  '{query}': {duration*1000:.1f}ms, {len(results)}æ¡ç»“æœ")
        
        # çŸ­æš‚ç­‰å¾…ç¡®ä¿ç¼“å­˜ç”Ÿæ•ˆ
        await asyncio.sleep(0.1)
        
        # ç¬¬äºŒè½®ï¼šç¼“å­˜å‘½ä¸­
        print("\nğŸš€ ç¬¬äºŒè½®æŸ¥è¯¢ (ç¼“å­˜å‘½ä¸­):")
        cached_times = []
        for query in test_queries:
            start_time = time.time()
            results = await self.manager.search_memories_concurrent(query, use_cache=True)
            duration = time.time() - start_time
            cached_times.append(duration)
            print(f"  '{query}': {duration*1000:.1f}ms, {len(results)}æ¡ç»“æœ")
        
        # åˆ†æç¼“å­˜æ•ˆæœ
        avg_uncached = statistics.mean(uncached_times)
        avg_cached = statistics.mean(cached_times)
        speedup = avg_uncached / avg_cached if avg_cached > 0 else 0
        
        print(f"\nğŸ“ˆ ç¼“å­˜ä¼˜åŒ–æ•ˆæœ:")
        print(f"  æœªç¼“å­˜å¹³å‡æ—¶é—´: {avg_uncached*1000:.1f}ms")
        print(f"  ç¼“å­˜å‘½ä¸­å¹³å‡æ—¶é—´: {avg_cached*1000:.1f}ms")
        print(f"  æ€§èƒ½æå‡: {speedup:.1f}x")
        print(f"  æ—¶é—´èŠ‚çœ: {((avg_uncached - avg_cached) / avg_uncached * 100):.1f}%")
        
        return {
            "avg_uncached_time": avg_uncached,
            "avg_cached_time": avg_cached,
            "speedup": speedup,
            "time_saved_percent": ((avg_uncached - avg_cached) / avg_uncached * 100) if avg_uncached > 0 else 0
        }
    
    async def demo_concurrent_processing(self):
        """æ¼”ç¤ºå¹¶å‘å¤„ç†èƒ½åŠ›"""
        print("\nâš¡ å¹¶å‘å¤„ç†æ¼”ç¤º")
        print("-" * 50)
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 5, 10, 20]
        results = {}
        
        for level in concurrency_levels:
            print(f"\nğŸ”„ æµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            
            async def single_request():
                query = "æ€§èƒ½ä¼˜åŒ–"
                start_time = time.time()
                await self.manager.search_memories_concurrent(query)
                return time.time() - start_time
            
            # æ‰§è¡Œå¹¶å‘è¯·æ±‚
            start_time = time.time()
            tasks = [single_request() for _ in range(level)]
            request_times = await asyncio.gather(*tasks)
            total_time = time.time() - start_time
            
            avg_request_time = statistics.mean(request_times)
            throughput = level / total_time
            
            results[level] = {
                "total_time": total_time,
                "avg_request_time": avg_request_time,
                "throughput": throughput
            }
            
            print(f"  æ€»è€—æ—¶: {total_time:.2f}s")
            print(f"  å¹³å‡è¯·æ±‚æ—¶é—´: {avg_request_time*1000:.1f}ms")
            print(f"  ååé‡: {throughput:.1f} è¯·æ±‚/ç§’")
        
        # åˆ†æå¹¶å‘æ•ˆæœ
        baseline_throughput = results[1]["throughput"]
        max_throughput = max(r["throughput"] for r in results.values())
        optimal_concurrency = max(results.keys(), key=lambda k: results[k]["throughput"])
        
        print(f"\nğŸ“Š å¹¶å‘å¤„ç†åˆ†æ:")
        print(f"  åŸºå‡†ååé‡ (å¹¶å‘=1): {baseline_throughput:.1f} è¯·æ±‚/ç§’")
        print(f"  æœ€å¤§ååé‡: {max_throughput:.1f} è¯·æ±‚/ç§’")
        print(f"  æœ€ä¼˜å¹¶å‘çº§åˆ«: {optimal_concurrency}")
        print(f"  å¹¶å‘æå‡å€æ•°: {max_throughput / baseline_throughput:.1f}x")
        
        return results
    
    async def demo_batch_processing(self):
        """æ¼”ç¤ºæ‰¹å¤„ç†ä¼˜åŒ–"""
        print("\nğŸ“¦ æ‰¹å¤„ç†ä¼˜åŒ–æ¼”ç¤º")
        print("-" * 50)
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        batch_conversations = []
        for i in range(20):
            project_context = {
                "project_name": f"batch-project-{i % 3}",
                "project_path": f"/demo/batch-{i % 3}",
                "project_type": "batch"
            }
            
            conversation_data = {
                "title": f"æ‰¹å¤„ç†å¯¹è¯ {i}",
                "messages": [
                    {"role": "user", "content": f"æ‰¹å¤„ç†é—®é¢˜ {i}"},
                    {"role": "assistant", "content": f"æ‰¹å¤„ç†å›ç­” {i}"}
                ],
                "metadata": {"batch_demo": True}
            }
            
            batch_conversations.append((conversation_data, project_context))
        
        # æµ‹è¯•é€ä¸ªå­˜å‚¨ vs æ‰¹é‡å­˜å‚¨
        print("ğŸŒ é€ä¸ªå­˜å‚¨æµ‹è¯•:")
        start_time = time.time()
        for conv_data, proj_context in batch_conversations[:10]:
            # æ¨¡æ‹Ÿé€ä¸ªå­˜å‚¨ï¼ˆå®é™…ä½¿ç”¨æ‰¹é‡æ¥å£ä½†æ¯æ¬¡åªå­˜ä¸€ä¸ªï¼‰
            await self.manager.store_conversation_batch([(conv_data, proj_context)])
        individual_time = time.time() - start_time
        
        print(f"  10ä¸ªå¯¹è¯é€ä¸ªå­˜å‚¨è€—æ—¶: {individual_time:.2f}s")
        
        print("\nğŸš€ æ‰¹é‡å­˜å‚¨æµ‹è¯•:")
        start_time = time.time()
        await self.manager.store_conversation_batch(batch_conversations[10:])
        batch_time = time.time() - start_time
        
        print(f"  10ä¸ªå¯¹è¯æ‰¹é‡å­˜å‚¨è€—æ—¶: {batch_time:.2f}s")
        
        speedup = individual_time / batch_time if batch_time > 0 else 0
        print(f"\nğŸ“ˆ æ‰¹å¤„ç†ä¼˜åŒ–æ•ˆæœ:")
        print(f"  æ‰¹å¤„ç†åŠ é€Ÿæ¯”: {speedup:.1f}x")
        print(f"  æ•ˆç‡æå‡: {((individual_time - batch_time) / individual_time * 100):.1f}%")
        
        return {
            "individual_time": individual_time,
            "batch_time": batch_time,
            "speedup": speedup
        }
    
    async def demo_performance_monitoring(self):
        """æ¼”ç¤ºæ€§èƒ½ç›‘æ§"""
        print("\nğŸ“Š æ€§èƒ½ç›‘æ§æ¼”ç¤º")
        print("-" * 50)
        
        # è·å–ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡
        perf_stats = await self.manager.get_performance_stats()
        
        print("ğŸ” å½“å‰ç³»ç»ŸçŠ¶æ€:")
        print(f"  å¹¶å‘ç»Ÿè®¡:")
        concurrent_stats = perf_stats["concurrent_stats"]
        print(f"    æ€»è¯·æ±‚æ•°: {concurrent_stats['total_requests']}")
        print(f"    å½“å‰å¹¶å‘: {concurrent_stats['concurrent_requests']}")
        print(f"    æœ€å¤§å¹¶å‘: {concurrent_stats['max_concurrent']}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {concurrent_stats['avg_response_time']*1000:.1f}ms")
        print(f"    é”™è¯¯æ•°: {concurrent_stats['error_count']}")
        
        print(f"\n  ç¼“å­˜ç»Ÿè®¡:")
        cache_stats = perf_stats["cache_stats"]
        print(f"    ç¼“å­˜å¤§å°: {cache_stats['cache_size']}/{cache_stats['max_size']}")
        print(f"    å‘½ä¸­æ¬¡æ•°: {cache_stats['hits']}")
        print(f"    æœªå‘½ä¸­æ¬¡æ•°: {cache_stats['misses']}")
        print(f"    å‘½ä¸­ç‡: {cache_stats['hit_rate']:.1%}")
        print(f"    æ·˜æ±°æ¬¡æ•°: {cache_stats['evictions']}")
        
        print(f"\n  è¿æ¥æ± ç»Ÿè®¡:")
        pool_stats = perf_stats["connection_pool"]
        print(f"    å½“å‰è¿æ¥æ•°: {pool_stats['total_connections']}")
        print(f"    æœ€å¤§è¿æ¥æ•°: {pool_stats['max_connections']}")
        print(f"    é˜Ÿåˆ—å¤§å°: {pool_stats['queue_size']}")
        
        # å¥åº·æ£€æŸ¥
        health_check = await self.manager.health_check_concurrent()
        print(f"\nğŸ¥ ç³»ç»Ÿå¥åº·æ£€æŸ¥:")
        print(f"  æ€»ä½“çŠ¶æ€: {health_check['status']}")
        for check_name, check_result in health_check["checks"].items():
            status_icon = "âœ…" if check_result["status"] == "ok" else "âŒ"
            print(f"  {check_name}: {status_icon} {check_result['status']}")
        
        return {
            "performance_stats": perf_stats,
            "health_check": health_check
        }
    
    async def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸ¯ Claude Memory MCP æ€§èƒ½ä¼˜åŒ–å®Œæ•´æ¼”ç¤º")
        print("=" * 80)
        
        try:
            # åˆå§‹åŒ–ç®¡ç†å™¨
            print("âš™ï¸  åˆå§‹åŒ–å¹¶å‘ä¼˜åŒ–è®°å¿†ç®¡ç†å™¨...")
            self.manager = create_concurrent_manager(self.config)
            await self.manager.initialize()
            print("âœ“ ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # è®¾ç½®æ¼”ç¤ºæ•°æ®
            num_conversations = await self.setup_demo_data()
            
            # æ‰§è¡Œå„é¡¹æ¼”ç¤º
            print(f"\nå¼€å§‹æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º (æ•°æ®é›†: {num_conversations}ä¸ªå¯¹è¯)...")
            
            # 1. ç¼“å­˜ä¼˜åŒ–æ¼”ç¤º
            cache_results = await self.demo_cache_optimization()
            
            # 2. å¹¶å‘å¤„ç†æ¼”ç¤º
            concurrent_results = await self.demo_concurrent_processing()
            
            # 3. æ‰¹å¤„ç†ä¼˜åŒ–æ¼”ç¤º
            batch_results = await self.demo_batch_processing()
            
            # 4. æ€§èƒ½ç›‘æ§æ¼”ç¤º
            monitoring_results = await self.demo_performance_monitoring()
            
            # ç”Ÿæˆæ¼”ç¤ºæ€»ç»“
            await self.generate_demo_summary({
                "cache_optimization": cache_results,
                "concurrent_processing": concurrent_results,
                "batch_processing": batch_results,
                "performance_monitoring": monitoring_results
            })
            
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        finally:
            if self.manager:
                await self.manager.close()
    
    async def generate_demo_summary(self, demo_data: Dict[str, Any]):
        """ç”Ÿæˆæ¼”ç¤ºæ€»ç»“"""
        print(f"\nğŸŠ æ€§èƒ½ä¼˜åŒ–æ¼”ç¤ºæ€»ç»“")
        print("=" * 80)
        
        cache_data = demo_data["cache_optimization"]
        concurrent_data = demo_data["concurrent_processing"]
        batch_data = demo_data["batch_processing"]
        
        print(f"âœ… ç¼“å­˜ä¼˜åŒ–æ•ˆæœ:")
        print(f"  ğŸš€ æŸ¥è¯¢åŠ é€Ÿ: {cache_data['speedup']:.1f}x")
        print(f"  â±ï¸  æ—¶é—´èŠ‚çœ: {cache_data['time_saved_percent']:.1f}%")
        print(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­æ˜¾è‘—æå‡æŸ¥è¯¢æ€§èƒ½")
        
        print(f"\nâœ… å¹¶å‘å¤„ç†èƒ½åŠ›:")
        baseline_throughput = concurrent_data[1]["throughput"]
        max_throughput = max(r["throughput"] for r in concurrent_data.values())
        optimal_level = max(concurrent_data.keys(), key=lambda k: concurrent_data[k]["throughput"])
        print(f"  ğŸ“ˆ æœ€å¤§ååé‡: {max_throughput:.1f} è¯·æ±‚/ç§’")
        print(f"  ğŸ”¢ æœ€ä¼˜å¹¶å‘çº§åˆ«: {optimal_level}")
        print(f"  âš¡ å¹¶å‘æå‡: {max_throughput / baseline_throughput:.1f}x")
        
        print(f"\nâœ… æ‰¹å¤„ç†ä¼˜åŒ–:")
        print(f"  ğŸ“¦ æ‰¹å¤„ç†åŠ é€Ÿ: {batch_data['speedup']:.1f}x")
        print(f"  ğŸ“Š æ•ˆç‡æå‡: {((batch_data['individual_time'] - batch_data['batch_time']) / batch_data['individual_time'] * 100):.1f}%")
        print(f"  ğŸ”§ æ‰¹é‡æ“ä½œæ˜¾è‘—å‡å°‘æ•°æ®åº“å¼€é”€")
        
        print(f"\nğŸ¯ æ ¸å¿ƒä¼˜åŒ–ç‰¹æ€§:")
        print(f"  ğŸŠâ€â™‚ï¸ è¿æ¥æ± ç®¡ç†: è‡ªåŠ¨è°ƒèŠ‚æ•°æ®åº“è¿æ¥ï¼Œæ”¯æŒé«˜å¹¶å‘")
        print(f"  ğŸ’¾ æ™ºèƒ½ç¼“å­˜: LRUç­–ç•¥ï¼Œè‡ªåŠ¨è¿‡æœŸæ¸…ç†ï¼Œæ˜¾è‘—æå‡æŸ¥è¯¢é€Ÿåº¦")
        print(f"  ğŸ“¦ æ‰¹å¤„ç†ä¼˜åŒ–: å‡å°‘æ•°æ®åº“I/Oï¼Œæå‡å†™å…¥æ€§èƒ½")
        print(f"  ğŸ“Š å®æ—¶ç›‘æ§: æ€§èƒ½æŒ‡æ ‡è¿½è¸ªï¼Œå¥åº·çŠ¶æ€æ£€æŸ¥")
        print(f"  ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–: æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´ç³»ç»Ÿå‚æ•°")
        
        print(f"\nğŸ’¡ å®é™…åº”ç”¨åœºæ™¯:")
        print(f"  ğŸ¢ ä¼ä¸šçº§éƒ¨ç½²: æ”¯æŒå¤šç”¨æˆ·å¹¶å‘è®¿é—®")
        print(f"  ğŸŒ å¤§è§„æ¨¡è®°å¿†æœç´¢: å¿«é€Ÿå“åº”è·¨é¡¹ç›®æŸ¥è¯¢")
        print(f"  ğŸ“ˆ é«˜è´Ÿè½½ç¯å¢ƒ: è‡ªåŠ¨æ‰©å±•å¤„ç†èƒ½åŠ›")
        print(f"  ğŸ”„ æŒç»­è¿è¡Œ: 7x24å°æ—¶ç¨³å®šæœåŠ¡")
        
        print(f"\nğŸ”§ æŠ€æœ¯äº®ç‚¹:")
        print(f"  âš¡ å¼‚æ­¥I/O: éé˜»å¡æ•°æ®åº“æ“ä½œ")
        print(f"  ğŸ”— è¿æ¥å¤ç”¨: WALæ¨¡å¼SQLiteä¼˜åŒ–")
        print(f"  ğŸ§  æ™ºèƒ½ç¼“å­˜: è‡ªé€‚åº”TTLå’ŒLRUç­–ç•¥")
        print(f"  ğŸ“Š æ€§èƒ½ç›‘æ§: å®æ—¶æŒ‡æ ‡æ”¶é›†å’Œåˆ†æ")
        print(f"  ğŸ›¡ï¸  å®¹é”™å¤„ç†: ä¼˜é›…é™çº§å’Œé”™è¯¯æ¢å¤")


async def main():
    """ä¸»å‡½æ•°"""
    demo = PerformanceOptimizationDemo()
    
    try:
        await demo.run_complete_demo()
        print(f"\nğŸ‰ Claude Memory MCP æ€§èƒ½ä¼˜åŒ–æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())